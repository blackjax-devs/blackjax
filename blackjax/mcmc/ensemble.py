# Copyright 2020- The Blackjax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Core functionality for ensemble MCMC algorithms."""
from typing import Callable, NamedTuple, Optional

import jax
import jax.numpy as jnp
from jax.flatten_util import ravel_pytree

from blackjax.base import SamplingAlgorithm
from blackjax.types import Array, ArrayTree, PRNGKey

__all__ = [
    "EnsembleState",
    "EnsembleInfo",
    "init",
    "build_kernel",
    "as_top_level_api",
    "stretch_move",
]


class EnsembleState(NamedTuple):
    """State of an ensemble sampler.

    coords
        An array or PyTree of arrays of shape `(n_walkers, ...)` that
        stores the current position of the walkers.
    log_probs
        An array of shape `(n_walkers,)` that stores the log-probability of
        each walker.
    blobs
        An optional PyTree that stores metadata returned by the log-probability
        function.
    """
    coords: ArrayTree
    log_probs: Array
    blobs: Optional[ArrayTree] = None


class EnsembleInfo(NamedTuple):
    """Additional information on the ensemble transition.

    acceptance_rate
        The acceptance rate of the ensemble.
    accepted
        A boolean array of shape `(n_walkers,)` indicating whether each walker's
        proposal was accepted.
    """
    acceptance_rate: Array
    accepted: Array


def stretch_move(
    rng_key: PRNGKey,
    walker_coords: ArrayTree,
    complementary_coords: ArrayTree,
    a: float = 2.0,
) -> tuple[ArrayTree, float]:
    """The emcee stretch move.

    A proposal is generated by selecting a random walker from the complementary
    ensemble and moving the current walker along the line connecting the two.
    """
    key_select, key_stretch = jax.random.split(rng_key)
    
    # Ravel coordinates to handle PyTrees
    walker_flat, unravel_fn = ravel_pytree(walker_coords)
    
    # Get the shape of the complementary ensemble
    # complementary_coords should have shape (n_walkers, ...) where ... matches walker_coords
    comp_leaves, comp_treedef = jax.tree_util.tree_flatten(complementary_coords)
    n_walkers_comp = comp_leaves[0].shape[0]
    
    # Select a random walker from the complementary ensemble
    idx = jax.random.randint(key_select, (), 0, n_walkers_comp)
    complementary_walker = jax.tree.map(lambda x: x[idx], complementary_coords)
    
    # Ravel the selected complementary walker
    complementary_walker_flat, _ = ravel_pytree(complementary_walker)
    
    # Generate the stretch factor `Z` from g(z)
    z = ((a - 1.0) * jax.random.uniform(key_stretch) + 1) ** 2.0 / a
    
    # Generate the proposal (Eq. 10)
    proposal_flat = complementary_walker_flat + z * (walker_flat - complementary_walker_flat)
    
    # The log of the Hastings ratio (Eq. 11)
    # Number of dimensions is the length of the flattened walker
    n_dims = walker_flat.shape[0]
    log_hastings_ratio = (n_dims - 1.0) * jnp.log(z)
    
    return unravel_fn(proposal_flat), log_hastings_ratio


def build_kernel(move_fn: Callable) -> Callable:
    """Builds a generic ensemble MCMC kernel."""

    def kernel(
        rng_key: PRNGKey, state: EnsembleState, logdensity_fn: Callable
    ) -> tuple[EnsembleState, EnsembleInfo]:
        
        n_walkers, *_ = jax.tree_util.tree_flatten(state.coords)[0][0].shape
        half_n = n_walkers // 2
        
        # Red-Blue Split
        walkers_red = jax.tree.map(lambda x: x[:half_n], state)
        walkers_blue = jax.tree.map(lambda x: x[half_n:], state)

        # Update Red walkers using Blue as complementary
        key_red, key_blue = jax.random.split(rng_key)
        new_walkers_red, accepted_red = _update_half(key_red, walkers_red, walkers_blue, logdensity_fn, move_fn)

        # Update Blue walkers using updated Red as complementary
        new_walkers_blue, accepted_blue = _update_half(key_blue, walkers_blue, new_walkers_red, logdensity_fn, move_fn)
        
        # Combine back
        new_coords = jax.tree.map(lambda r, b: jnp.concatenate([r, b], axis=0), new_walkers_red.coords, new_walkers_blue.coords)
        new_log_probs = jnp.concatenate([new_walkers_red.log_probs, new_walkers_blue.log_probs])
        
        if state.blobs is not None:
            new_blobs = jax.tree.map(lambda r, b: jnp.concatenate([r, b], axis=0), new_walkers_red.blobs, new_walkers_blue.blobs)
        else:
            new_blobs = None

        new_state = EnsembleState(new_coords, new_log_probs, new_blobs)
        accepted = jnp.concatenate([accepted_red, accepted_blue])
        acceptance_rate = jnp.mean(accepted.astype(jnp.float32))
        info = EnsembleInfo(acceptance_rate, accepted)

        return new_state, info

    return kernel


def _update_half(rng_key, walkers_to_update, complementary_walkers, logdensity_fn, move_fn):
    """Helper to update one half of the ensemble."""
    n_update, *_ = jax.tree_util.tree_flatten(walkers_to_update.coords)[0][0].shape
    keys = jax.random.split(rng_key, n_update)

    # Vectorize the move over the walkers to be updated
    proposals, log_hastings_ratios = jax.vmap(
        lambda k, w_coords: move_fn(k, w_coords, complementary_walkers.coords)
    )(keys, walkers_to_update.coords)
    
    # Compute log-probabilities for proposals
    logdensity_outputs = jax.vmap(logdensity_fn)(proposals)
    if isinstance(logdensity_outputs, tuple):
        log_probs_proposal, blobs_proposal = logdensity_outputs
    else:
        log_probs_proposal = logdensity_outputs
        blobs_proposal = None
    
    # MH accept/reject step (Eq. 11)
    log_p_accept = log_hastings_ratios + log_probs_proposal - walkers_to_update.log_probs
    
    # To avoid -inf - (-inf) = NaN, replace -inf with a large negative number.
    log_p_accept = jnp.where(jnp.isneginf(walkers_to_update.log_probs), -jnp.inf, log_p_accept)
    
    u = jax.random.uniform(rng_key, shape=(n_update,))
    accepted = jnp.log(u) < log_p_accept

    # Build the new state for the half
    new_coords = jax.tree.map(lambda prop, old: jnp.where(accepted[:, None], prop, old), proposals, walkers_to_update.coords)
    new_log_probs = jnp.where(accepted, log_probs_proposal, walkers_to_update.log_probs)
    
    if walkers_to_update.blobs is not None:
        new_blobs = jax.tree.map(
            lambda prop, old: jnp.where(accepted, prop, old),
            blobs_proposal,
            walkers_to_update.blobs,
        )
    else:
        new_blobs = None
    
    new_walkers = EnsembleState(new_coords, new_log_probs, new_blobs)
    return new_walkers, accepted


def init(position: ArrayTree, logdensity_fn: Callable, has_blobs: bool = False) -> EnsembleState:
    """Initializes the ensemble."""
    logdensity_outputs = jax.vmap(logdensity_fn)(position)
    if isinstance(logdensity_outputs, tuple):
        log_probs, blobs = logdensity_outputs
        return EnsembleState(position, log_probs, blobs)
    else:
        log_probs = logdensity_outputs
        return EnsembleState(position, log_probs, None)


def as_top_level_api(
    logdensity_fn: Callable, move_fn: Callable, has_blobs: bool = False
) -> SamplingAlgorithm:
    """Implements the user-facing API for ensemble samplers."""
    kernel = build_kernel(move_fn)

    def init_fn(position: ArrayTree, rng_key=None):
        return init(position, logdensity_fn, has_blobs)

    def step_fn(rng_key: PRNGKey, state: EnsembleState):
        return kernel(rng_key, state, logdensity_fn)

    return SamplingAlgorithm(init_fn, step_fn)